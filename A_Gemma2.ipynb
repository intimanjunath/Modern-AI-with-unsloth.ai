{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "EXd_h9f_iFGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth accelerate peft trl transformers bitsandbytes datasets"
      ],
      "metadata": {
        "id": "9_ytnOj2huS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Load Gemma 2 (9B) with Unsloth"
      ],
      "metadata": {
        "id": "W6e2VlFViFru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2b-bnb-4bit\",  # we use 2B because Unsloth hasn't released 9B yet\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "be70957c58d04116bc9f2a33a6dbe5b2",
            "5ce375e390694221ba0b0a65cc2de835",
            "cd64c6a1a01544329bed198f0e81e522",
            "4f4566811f2f43819cb0acb1f54e6e9c",
            "905169f90af642ce9d2cc59cf968e6c5",
            "1ca59f089aed4b569d613d1410f06dd3",
            "91aa0a21e6104728bec9107c7471ddbe",
            "3e2ed4122f4f4e849a25ff3273ddc989",
            "9449cf66585148b1ab61cd8a2aea07cc",
            "760c4fba98fb41bab0e76bdf9c51f368",
            "069b2169e5234ed9930b415aabd7f096",
            "85a69ee8fa954af1936f16237e5744f6",
            "d008755c38744c0da28acb47e4c12bda",
            "693ed74d82614935b034499bd93a684f",
            "81a7be044c8644b7be55bc7ceac58243",
            "2ab235f3870143abac09cf5f92600740",
            "16e99582899841c290685798ba521930",
            "59cd126498d4438eb0d6df5ae54b1fa3",
            "91d74e4427dd4058a5d4de552624e2e2",
            "82b628c54d994264bdabbb5e7de05465",
            "db011436b3204cb2b302f155ce47d5db",
            "daca7af868bf4f289975c52de1bd4e15",
            "3d74acfef785417fb9118d6ffbd3c104",
            "73ba43f2f5bb4e7f9958930dedacbb87",
            "42866f561b304328bc7eb4ba9315769e",
            "b897d6d83885410b9d7ff41db57bea0b",
            "9b6fe3edac464c2584fbca8eccd69809",
            "02827aa0ecf54551b621ba4547719c47",
            "2809a408e2a5472284fcce47f230639a",
            "26963868b59d4deebf554781e1360351",
            "a0b3635891de412483fb076497166e72",
            "37d6314d651e45e897d0c1ab89dcf4e1",
            "91a16e03defe4af39d8533813c247df9",
            "0edddd7227f941509803b221a83c030c",
            "d4c6ebbcb4e440a18944785ba93fc58a",
            "9c91eaed639940e4917e2841bfefcf56",
            "1e7c684c76324e7fa3141c427df26dbb",
            "e942d38681e94479b3d34db165a9320c",
            "4b77819684f842d89d49fcb90d6d0536",
            "9412360303a14d13bcc700d8dc567798",
            "dcd1388766144e50b79b8b9d7ad398bd",
            "9a97c97b8567423ebc2598121ba9f4ae",
            "72e540cc2ba2490595acfe2d6ff7db57",
            "53f2dfc8835d44209fba7da756f84d39",
            "1b579d71dbfa45538e3afa492185a813",
            "a96d5b0350844a0f8eb6a2b2c84ae7dd",
            "0a0aa06cb47a48dc816e3843523fc61c",
            "cae72b8a0b9440caa625d920ae47e59d",
            "098dcd4bbb4940ab87a50bf24b981d99",
            "fbd7ad4f1321401fbc4763f447055ed8",
            "353c5e151b194544a9e6615ec9adfcc4",
            "5d54d228992b438ca562b377b9fc42d8",
            "be3b46d6d1444dac9410473035efccc7",
            "3c4583a7428a4d2dbae8de794df29a46",
            "8427c1533bce4f4d86690875b60e04c4",
            "f3c59515b6cb4313a3bab3251718e5ed",
            "ee6ecb45e1b94a479da07f491090dcc2",
            "809a71fd1c574a94ae3f37ff69afef76",
            "12f8a80ecad44ea08400aab0cb0aa6d5",
            "7a2a05b29af9483aafbde162dd7bac70",
            "452202a93be7466cabf5f512df91f6a0",
            "f589c1b396074880a3e13af7e602f940",
            "4b5e6cf7ee8648f6bc8cfd8aa232e44a",
            "30a7675950b34cf48ceaf46985c59dcd",
            "7cc221d2ec794263b66cbf4164a0fb96",
            "7a7d3e11d2724617b6e3a26d63f71fed"
          ]
        },
        "id": "eC9T3WdWiA_V",
        "outputId": "5be8829f-dce7-4c5a-eb09-62f2ed427ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Gemma patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.07G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be70957c58d04116bc9f2a33a6dbe5b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85a69ee8fa954af1936f16237e5744f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d74acfef785417fb9118d6ffbd3c104"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0edddd7227f941509803b221a83c030c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b579d71dbfa45538e3afa492185a813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3c59515b6cb4313a3bab3251718e5ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Create a Small QA Dataset"
      ],
      "metadata": {
        "id": "BebTQkT_iTh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "qa_data = [\n",
        "    {\"instruction\": \"What is the capital of France?\", \"input\": \"\", \"output\": \"The capital of France is Paris.\"},\n",
        "    {\"instruction\": \"Who wrote 'Pride and Prejudice'?\", \"input\": \"\", \"output\": \"Jane Austen wrote 'Pride and Prejudice'.\"},\n",
        "    {\"instruction\": \"What is the largest planet in our solar system?\", \"input\": \"\", \"output\": \"Jupiter is the largest planet in our solar system.\"},\n",
        "    {\"instruction\": \"What is the chemical symbol for water?\", \"input\": \"\", \"output\": \"The chemical symbol for water is H₂O.\"},\n",
        "    {\"instruction\": \"Who painted the Mona Lisa?\", \"input\": \"\", \"output\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_list(qa_data)"
      ],
      "metadata": {
        "id": "5nUkDXoYh7iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Format Chat Prompt"
      ],
      "metadata": {
        "id": "T279dyUciXU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_qa(example):\n",
        "    prompt = f\"\"\"<|start_of_text|><|user|>\\n{example[\"instruction\"]}{example[\"input\"]}<|end_of_turn|><|assistant|>\\n{example[\"output\"]}<|end_of_turn|>\"\"\"\n",
        "    example[\"text\"] = prompt\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(format_qa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9daec0f4630e4217955d971d760ab228",
            "c06195d282d5410b89e1686a1c9577d6",
            "d8b9cb3c8b394f6098e8254cd7148498",
            "0f16cdb2b18c45569d8bb931d1a9d6dc",
            "dbc2edc39869407981450f2311893426",
            "bb6f3ba1de3247ba9ab2de6b58598b74",
            "8076835e7ad7489ba9e2b18945698b21",
            "5b0ecb553e134851be142535317995e1",
            "6db08a9462ed402c94ea7c349cad6a45",
            "1efda3f905134f32a0f9f820acb7abb4",
            "7034d23572ca4b7ba33de5ce78fcab65"
          ]
        },
        "id": "pr26AVHNh99r",
        "outputId": "1b7366ec-c316-4a0f-a494-f8aec748eef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9daec0f4630e4217955d971d760ab228"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Prepare LoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "dZlf-xKfibNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsUeAo4Ph2Sy",
        "outputId": "d2f8d0b4-394d-4f08-eb5b-61545b20d94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Train the Model"
      ],
      "metadata": {
        "id": "s_rloBr5ieUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=30,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    output_dir=\"gemma2_qa_lora\",\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "c090e42a03d040a3ab63c0cd0bfcf7a0",
            "f1ccdbba1f07481197f7eeb4f17ef2bc",
            "13efbc24a6b54900a285445b4290b200",
            "44e35d7665b343eaa8652ee54ce0a7fe",
            "8cc578f0faa0450c8eb519e947303a0f",
            "f0618d71cdd54631b3b6496d5928bfa3",
            "aa932b40a16447988c9288df04f149e2",
            "0b6458b2dcb8429d97a50be2ee369cfb",
            "0fb22430e1aa4e7e86e33dcd93071565",
            "540998b7a5bd4bb9825c44194571cbde",
            "d897bf7f4c2442688f48f6ca8bd8f70a"
          ]
        },
        "id": "jNjgs4BlhzaJ",
        "outputId": "af70b224-145d-44a5-8410-a2db2ca0bfe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c090e42a03d040a3ab63c0cd0bfcf7a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 5 | Num Epochs = 30 | Total steps = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 3,686,400/2,000,000,000 (0.18% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='0' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 0/30 : < :, Epoch 0/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=0, training_loss=11939.319610595703, metrics={'train_runtime': 15.1205, 'train_samples_per_second': 15.873, 'train_steps_per_second': 1.984, 'total_flos': 36907790327808.0, 'train_loss': 11939.319610595703})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Inference Cell"
      ],
      "metadata": {
        "id": "4-XLKjaaiilL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"<|start_of_text|><|user|>\n",
        "Please answer the following question directly: What is the largest planet in our solar system?<|end_of_turn|><|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "eos_token_id = tokenizer.convert_tokens_to_ids(\"<|end_of_turn|>\") or tokenizer.eos_token_id\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "# Extract the assistant's answer cleanly\n",
        "if \"<|assistant|>\" in decoded:\n",
        "    response = decoded.split(\"<|assistant|>\")[-1].split(\"<|\")[0].strip()\n",
        "    print(\"=== Assistant Response ===\\n\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"⚠️ Could not parse output. Raw result:\\n\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70AQUfkZhwUR",
        "outputId": "bc7bee3b-a058-4127-fd6a-7818a5ee785b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Assistant Response ===\n",
            "\n",
            "What is the distance from Earth to Jupiter?\n"
          ]
        }
      ]
    }
  ]
}