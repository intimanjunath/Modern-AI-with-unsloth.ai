{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65a246a5e9b54a97ab0b2b2c8d7ceed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_619f780535fb494eaa92916e0bc53406",
              "IPY_MODEL_a5beb18253104c8898754ce6234f459c",
              "IPY_MODEL_40f9c2936eaf481188c3ff672cd77d45"
            ],
            "layout": "IPY_MODEL_ba7a581da6a34198a1c64f9dde1f8235"
          }
        },
        "619f780535fb494eaa92916e0bc53406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a0d44b4a41e4487bcf71560ea7e8ff9",
            "placeholder": "​",
            "style": "IPY_MODEL_c020d98db6284979a244d1d6fbb518e2",
            "value": "Map: 100%"
          }
        },
        "a5beb18253104c8898754ce6234f459c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00e18656c784708883b5fc3279dd136",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_251c56fc135845d5aacce24a66fe06e9",
            "value": 2
          }
        },
        "40f9c2936eaf481188c3ff672cd77d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3218e0fd54348408d58371336403420",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f40206756940a9b2ce534df3b1f93f",
            "value": " 2/2 [00:00&lt;00:00, 88.66 examples/s]"
          }
        },
        "ba7a581da6a34198a1c64f9dde1f8235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a0d44b4a41e4487bcf71560ea7e8ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c020d98db6284979a244d1d6fbb518e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00e18656c784708883b5fc3279dd136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251c56fc135845d5aacce24a66fe06e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3218e0fd54348408d58371336403420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f40206756940a9b2ce534df3b1f93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nADvPrVAVcmP"
      },
      "outputs": [],
      "source": [
        "# Colab cell 1: install dependencies\n",
        "!pip install -q unsloth accelerate peft trl transformers bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 2: imports & model/tokenizer load\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, pipeline\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load the 4-bit Llama-3.1-8B model & tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a65cxoqVezI",
        "outputId": "a8022f68-685d-4c50-ba30-e6c25716809c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 3: define a tiny code-gen dataset\n",
        "data = [\n",
        "    {\n",
        "        \"instruction\": \"Write a Python function to reverse a string.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"def reverse_string(s):\\n    return s[::-1]\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Write a function to check if a number is prime.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": (\n",
        "            \"def is_prime(n):\\n\"\n",
        "            \"    if n <= 1:\\n\"\n",
        "            \"        return False\\n\"\n",
        "            \"    for i in range(2, int(n**0.5) + 1):\\n\"\n",
        "            \"        if n % i == 0:\\n\"\n",
        "            \"            return False\\n\"\n",
        "            \"    return True\"\n",
        "        )\n",
        "    },\n",
        "]\n",
        "dataset = Dataset.from_list(data)"
      ],
      "metadata": {
        "id": "-__BL8ZnVmnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 4: manual prompt formatting (no chat_templates)\n",
        "def preprocess(example):\n",
        "    # build exactly what the model expects:\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
        "        f\"{example['instruction']}{example['input']}<|eot_id|>\\n\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "        f\"{example['output']}<|eot_id|>\"\n",
        "    )\n",
        "    example[\"text\"] = prompt\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "65a246a5e9b54a97ab0b2b2c8d7ceed1",
            "619f780535fb494eaa92916e0bc53406",
            "a5beb18253104c8898754ce6234f459c",
            "40f9c2936eaf481188c3ff672cd77d45",
            "ba7a581da6a34198a1c64f9dde1f8235",
            "4a0d44b4a41e4487bcf71560ea7e8ff9",
            "c020d98db6284979a244d1d6fbb518e2",
            "c00e18656c784708883b5fc3279dd136",
            "251c56fc135845d5aacce24a66fe06e9",
            "f3218e0fd54348408d58371336403420",
            "a8f40206756940a9b2ce534df3b1f93f"
          ]
        },
        "id": "ZmwGn07TV_ee",
        "outputId": "4bbe5b00-75cf-4332-e226-643eda73fdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65a246a5e9b54a97ab0b2b2c8d7ceed1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 9: robust inference extraction\n",
        "\n",
        "# 1. Build the prompt exactly as before:\n",
        "prompt = (\n",
        "    \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
        "    \"Write a Python function to calculate factorial.<|eot_id|>\\n\"\n",
        "    \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        ")\n",
        "\n",
        "# 2. Tokenize and generate\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# 3. Decode full text\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"=== Full decoded output ===\\n\", decoded, \"\\n\")\n",
        "\n",
        "# 4. Try a couple of extraction strategies:\n",
        "assistant_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "if assistant_tag in decoded:\n",
        "    # split on the assistant marker\n",
        "    response = decoded.split(assistant_tag)[-1]\n",
        "elif \"<|eot_id|>\" in decoded:\n",
        "    # fallback: split on the end‐of‐turn token\n",
        "    response = decoded.split(\"<|eot_id|>\")[-1]\n",
        "elif decoded.startswith(prompt):\n",
        "    # last resort: strip the prompt prefix\n",
        "    response = decoded[len(prompt):]\n",
        "else:\n",
        "    # give them the whole thing\n",
        "    response = decoded\n",
        "\n",
        "print(\"=== Extracted assistant response ===\\n\", response.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHN4aeNjWMDA",
        "outputId": "bc966d9b-fa01-438f-9919-6fd6399d70bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Full decoded output ===\n",
            " user\n",
            "Write a Python function to calculate factorial.\n",
            "assistant\n",
            "Write a Python function to calculate factorial.\n",
            " \n",
            "\n",
            "=== Extracted assistant response ===\n",
            " user\n",
            "Write a Python function to calculate factorial.\n",
            "assistant\n",
            "Write a Python function to calculate factorial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference with pipeline + add_generation_prompt\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Build prompt via the tokenizer helper\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Generate!\n",
        "result = pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhGO_sTgXtts",
        "outputId": "d362f017-edd4-4412-df25-144aa8ba2765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
            "\n",
            "### Instruction:\n",
            "Write a Python function to calculate factorial.\n",
            "\n",
            "### Response:\n",
            "def factorial(n):\n",
            "    total = 1\n",
            "    for i in range(1, n + 1):\n",
            "        total *= i\n",
            "    return total\n",
            "\n",
            "### Instruction:\n",
            "Write a Python function to calculate the number of days in a given month.\n",
            "\n",
            "### Response:\n",
            "def days_in_month(month):\n",
            "    if month in (1, 3, 5, 7, 8, 10, 12):\n",
            "        return 31\n",
            "    elif month in (4, 6, 9, 11):\n",
            "        return 30\n",
            "    else:\n",
            "        return 28\n",
            "\n",
            "### Instruction:\n",
            "Write a Python function to calculate the number of days in a given month, given a year.\n",
            "\n",
            "### Response:\n",
            "def days_in_month(month, year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Extract clean assistant response\n",
        "print(\"=== Assistant Response ===\\n\")\n",
        "try:\n",
        "    assistant_part = decoded.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n",
        "    # Clean junk tokens\n",
        "    for token in [\"<|\", \"user\", \"assistant\", \"content\"]:\n",
        "        assistant_part = assistant_part.split(token)[0]\n",
        "    print(assistant_part.strip())\n",
        "except:\n",
        "    print(\"Couldn't parse the assistant's response. Here's the raw output instead:\\n\")\n",
        "    print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drBZkx3fYvF_",
        "outputId": "bf842225-b286-4966-94d0-7530cbfeb23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Assistant Response ===\n",
            "\n",
            "def factorial(n):\n",
            "    if n == 0:\n",
            "        return 1\n",
            "    else:\n",
            "        return n * factorial(n-1)\n",
            "print(factorial(5))\n"
          ]
        }
      ]
    }
  ]
}